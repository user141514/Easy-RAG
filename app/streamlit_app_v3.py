# æ–‡ä»¶: app/streamlit_app_v3.py
# RAG v3.0 - çœŸæ­£èƒ½æ€è€ƒçš„ç‰ˆæœ¬

import streamlit as st
import tempfile
import os
import gc
import time
import uuid
import re

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate

# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="RAG v3.0 - æ™ºèƒ½ç‰ˆ",
    page_icon="ğŸ§ ",
    layout="wide"
)

# ==================== é…ç½® ====================
VECTORDB_BASE_PATH = r"D:\local-rag-chatbot\data\vectordb"
OLLAMA_BASE_URL = "http://localhost:11434"
LLM_MODEL = "llama3:8b"
EMBEDDING_MODEL = "nomic-embed-text"
MAX_HISTORY_TURNS = 5

# ==================== Session State ====================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False
if "all_chunks" not in st.session_state:
    st.session_state.all_chunks = []
if "debug_info" not in st.session_state:
    st.session_state.debug_info = {}


# ==================== æ ¸å¿ƒæ¨¡å‹ ====================

@st.cache_resource
def get_llm():
    return OllamaLLM(
        model=LLM_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=0.3  # é™ä½éšæœºæ€§ï¼Œæ›´ç¨³å®š
    )


@st.cache_resource
def get_embeddings():
    return OllamaEmbeddings(
        model=EMBEDDING_MODEL,
        base_url=OLLAMA_BASE_URL
    )


# ==================== å·¥å…·å‡½æ•° ====================

def cleanup_old_vectorstore():
    if st.session_state.vectorstore is not None:
        try:
            st.session_state.vectorstore = None
            gc.collect()
            time.sleep(0.5)
        except:
            pass


def process_pdf(uploaded_file):
    """å¤„ç† PDF"""
    cleanup_old_vectorstore()

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    try:
        loader = PyPDFLoader(tmp_path)
        pages = loader.load()

        # æ›´ç»†çš„åˆ†å—ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # æ›´å°çš„å—
            chunk_overlap=100,  # æ›´å¤šé‡å 
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
        )
        chunks = text_splitter.split_documents(pages)

        db_path = f"{VECTORDB_BASE_PATH}_{uuid.uuid4().hex[:8]}"

        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=get_embeddings(),
            persist_directory=db_path
        )

        return vectorstore, pages, chunks
    finally:
        try:
            os.unlink(tmp_path)
        except:
            pass


# ==================== æ™ºèƒ½æ£€ç´¢æ¨¡å— ====================

def expand_query(original_question: str) -> list:
    """
    æŸ¥è¯¢æ‰©å±•ï¼šè®© LLM ç”Ÿæˆå¤šä¸ªæœç´¢è§’åº¦
    """
    prompt = f"""ç”¨æˆ·é—®é¢˜: "{original_question}"

è¯·ä»ä¸åŒè§’åº¦ç”Ÿæˆ 3 ä¸ªæœç´¢å…³é”®è¯/çŸ­è¯­ï¼Œç”¨äºæ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚
è¦æ±‚ï¼š
1. è¦†ç›–é—®é¢˜çš„ä¸åŒæ–¹é¢
2. åŒ…å«åŒä¹‰è¯æˆ–ç›¸å…³æ¦‚å¿µ
3. æ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Š

å…³é”®è¯ï¼š"""

    result = get_llm().invoke(prompt)

    # è§£æå…³é”®è¯
    queries = [original_question]  # åŸå§‹é—®é¢˜
    for line in result.strip().split('\n'):
        line = line.strip().strip('-â€¢*1234567890.').strip()
        if line and 2 < len(line) < 50:
            queries.append(line)

    return queries[:4]  # æœ€å¤š 4 ä¸ªæŸ¥è¯¢


def smart_retrieve(question: str, vectorstore, top_k: int = 5) -> list:
    """
    æ™ºèƒ½æ£€ç´¢ï¼šå¤šæŸ¥è¯¢ + å»é‡ + æ’åº
    """
    # 1. æ‰©å±•æŸ¥è¯¢
    queries = expand_query(question)

    # 2. å¤šæ¬¡æ£€ç´¢
    all_docs = []
    seen_content = set()
    doc_scores = {}

    for i, query in enumerate(queries):
        # å¸¦åˆ†æ•°çš„æ£€ç´¢
        results = vectorstore.similarity_search_with_score(query, k=top_k)

        for doc, score in results:
            content_key = doc.page_content[:100]

            if content_key not in seen_content:
                seen_content.add(content_key)
                all_docs.append(doc)
                doc_scores[content_key] = score
            else:
                # å¦‚æœé‡å¤å‡ºç°ï¼Œé™ä½åˆ†æ•°ï¼ˆè¯´æ˜æ›´ç›¸å…³ï¼‰
                doc_scores[content_key] = min(doc_scores[content_key], score)

    # 3. æŒ‰åˆ†æ•°æ’åºï¼ˆåˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼‰
    all_docs.sort(key=lambda d: doc_scores.get(d.page_content[:100], 999))

    return all_docs[:8], queries  # è¿”å›æœ€ç›¸å…³çš„ 8 ä¸ª


# ==================== æ€è€ƒå‹å›ç­”æ¨¡å— ====================

def format_history(chat_history, max_turns=3):
    """æ ¼å¼åŒ–å†å²å¯¹è¯"""
    if not chat_history:
        return "æ— "

    recent = chat_history[-max_turns:]
    lines = []
    for chat in recent:
        lines.append(f"ç”¨æˆ·: {chat['question']}")
        answer_preview = chat['answer'][:150] + "..." if len(chat['answer']) > 150 else chat['answer']
        lines.append(f"åŠ©æ‰‹: {answer_preview}")
    return "\n".join(lines)


def thinking_answer(question: str, docs: list, chat_history: list) -> str:
    """
    æ€è€ƒå‹å›ç­”ï¼šè®© LLM çœŸæ­£åˆ†æ
    """
    context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    history = format_history(chat_history)

    # æ ¸å¿ƒï¼šå¼•å¯¼ LLM æ€è€ƒçš„ Prompt
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆ†æé¡¾é—®ã€‚ç”¨æˆ·åŸºäºä¸€ä»½æ–‡æ¡£å‘ä½ æé—®ã€‚

## å‚è€ƒèµ„æ–™
{context}

## å¯¹è¯å†å²
{history}

## ç”¨æˆ·é—®é¢˜
{question}

---

## è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ€è€ƒå’Œå›ç­”ï¼š

### ç¬¬ä¸€æ­¥ï¼šç†è§£é—®é¢˜
è¿™ä¸ªé—®é¢˜çœŸæ­£æƒ³é—®çš„æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆéšå«çš„éœ€æ±‚ï¼Ÿ

### ç¬¬äºŒæ­¥ï¼šä¿¡æ¯æå–
ä»å‚è€ƒèµ„æ–™ä¸­ï¼Œæ‰¾å‡ºä¸é—®é¢˜ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚åˆ—å‡ºè¦ç‚¹ã€‚

### ç¬¬ä¸‰æ­¥ï¼šåˆ†ææ¨ç†
åŸºäºè¿™äº›ä¿¡æ¯ï¼Œè¿›è¡Œåˆ†æã€‚
- å¦‚æœæ˜¯äº‹å®æ€§é—®é¢˜ï¼šæå–å¹¶æ•´ç†äº‹å®
- å¦‚æœæ˜¯åˆ†ææ€§é—®é¢˜ï¼šç»™å‡ºä½ çš„åˆ†æé€»è¾‘
- å¦‚æœæ˜¯æ¯”è¾ƒæ€§é—®é¢˜ï¼šåˆ—å‡ºå¯¹æ¯”è¦ç‚¹
- å¦‚æœæ˜¯å»ºè®®æ€§é—®é¢˜ï¼šç»™å‡ºæœ‰ä¾æ®çš„å»ºè®®

### ç¬¬å››æ­¥ï¼šå›ç­”
ç»™å‡ºæœ€ç»ˆå›ç­”ã€‚è¦æ±‚ï¼š
- ç›´æ¥å›åº”é—®é¢˜
- è¨€ä¹‹æœ‰ç‰©ï¼Œä¸è¯´åºŸè¯
- å¦‚æœèµ„æ–™ä¸è¶³ä»¥å®Œæ•´å›ç­”ï¼Œæ˜ç¡®è¯´æ˜ç¼ºå°‘ä»€ä¹ˆä¿¡æ¯

---

ç°åœ¨å¼€å§‹ä½ çš„åˆ†æï¼š"""

    return get_llm().invoke(prompt)


def check_answer_quality(question: str, answer: str) -> tuple:
    """
    æ£€æŸ¥å›ç­”è´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦æ”¹è¿›
    """
    check_prompt = f"""è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ï¼š

é—®é¢˜ï¼š{question}

å›ç­”ï¼š{answer}

è¯·è¯„ä¼°ï¼ˆåªå›å¤ä¸€ä¸ªè¯ï¼‰ï¼š
- å¦‚æœå›ç­”ç›´æ¥å›åº”äº†é—®é¢˜ä¸”æœ‰å®è´¨å†…å®¹ï¼Œå›å¤ï¼šGOOD
- å¦‚æœå›ç­”ç©ºæ´ã€æ²¡æœ‰å®è´¨å†…å®¹æˆ–å®Œå…¨è·‘é¢˜ï¼Œå›å¤ï¼šBAD

è¯„ä¼°ï¼š"""

    result = get_llm().invoke(check_prompt).strip().upper()
    is_good = "GOOD" in result
    return is_good, result


def improve_answer(question: str, original_answer: str, docs: list) -> str:
    """
    æ”¹è¿›ä¸å¤Ÿå¥½çš„å›ç­”
    """
    context = "\n\n".join([doc.page_content for doc in docs])

    prompt = f"""åŸå§‹é—®é¢˜ï¼š{question}

å‚è€ƒèµ„æ–™ï¼š
{context}

ä¹‹å‰çš„å›ç­”ï¼ˆè´¨é‡ä¸å¤Ÿå¥½ï¼‰ï¼š
{original_answer}

è¯·é‡æ–°å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›åº”é—®é¢˜æ ¸å¿ƒ
2. ä»å‚è€ƒèµ„æ–™ä¸­æå–å…·ä½“ä¿¡æ¯
3. å¦‚æœèµ„æ–™ä¸­ç¡®å®æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥è¯´æ˜
4. ç®€æ´æœ‰åŠ›ï¼Œä¸è¦ç©ºè¯

æ”¹è¿›åçš„å›ç­”ï¼š"""

    return get_llm().invoke(prompt)


# ==================== ä¸»å›ç­”å‡½æ•° ====================

def smart_rag_answer(question: str, vectorstore, chat_history: list, debug=False):
    """
    å®Œæ•´çš„æ™ºèƒ½ RAG æµç¨‹
    """
    debug_info = {}

    # 1. æ™ºèƒ½æ£€ç´¢
    docs, queries = smart_retrieve(question, vectorstore)
    debug_info["search_queries"] = queries
    debug_info["docs_found"] = len(docs)

    if not docs:
        return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", [], debug_info

    # 2. æ€è€ƒå‹å›ç­”
    answer = thinking_answer(question, docs, chat_history)
    debug_info["first_answer"] = answer[:200] + "..."

    # 3. è´¨é‡æ£€æŸ¥
    is_good, quality = check_answer_quality(question, answer)
    debug_info["quality_check"] = quality

    # 4. å¦‚æœä¸å¤Ÿå¥½ï¼Œæ”¹è¿›
    if not is_good:
        answer = improve_answer(question, answer, docs)
        debug_info["improved"] = True
    else:
        debug_info["improved"] = False

    return answer, docs, debug_info


# ==================== ç®€æ´å›ç­”æ¨¡å¼ ====================

def concise_answer(question: str, docs: list, chat_history: list) -> str:
    """
    ç®€æ´å›ç­”æ¨¡å¼ï¼šç›´æ¥ç»™ç­”æ¡ˆ
    """
    context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    history = format_history(chat_history)

    prompt = f"""å‚è€ƒèµ„æ–™ï¼š
{context}

å¯¹è¯å†å²ï¼š
{history}

é—®é¢˜ï¼š{question}

è¯·ç›´æ¥ã€ç®€æ´åœ°å›ç­”é—®é¢˜ã€‚
- å¦‚æœèƒ½ä»èµ„æ–™ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œç›´æ¥ç»™å‡º
- å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯´"æ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
- ä¸è¦è¯´å¤šä½™çš„è¯

å›ç­”ï¼š"""

    return get_llm().invoke(prompt)


# ==================== é¡µé¢å¸ƒå±€ ====================

st.title("ğŸ§  RAG v3.0 - æ™ºèƒ½ç‰ˆ")
st.caption("å¤šè§’åº¦æ£€ç´¢ + æ€è€ƒå‹å›ç­” + è´¨é‡è‡ªæ£€")

col1, col2 = st.columns([1, 2])

# ==================== å·¦ä¾§æ§åˆ¶é¢æ¿ ====================
with col1:
    st.header("ğŸ“ æ–‡æ¡£")

    uploaded_file = st.file_uploader("ä¸Šä¼  PDF", type=["pdf"])

    if uploaded_file:
        if st.button("ğŸš€ å¤„ç†æ–‡æ¡£", type="primary", use_container_width=True):
            with st.spinner("å¤„ç†ä¸­..."):
                try:
                    vectorstore, pages, chunks = process_pdf(uploaded_file)
                    st.session_state.vectorstore = vectorstore
                    st.session_state.all_chunks = chunks
                    st.session_state.pdf_processed = True
                    st.session_state.chat_history = []
                    st.success(f"âœ… {len(pages)} é¡µ / {len(chunks)} å—")
                except Exception as e:
                    st.error(f"å¤±è´¥: {e}")

    st.divider()

    # å›ç­”æ¨¡å¼é€‰æ‹©
    st.header("âš™ï¸ è®¾ç½®")
    answer_mode = st.radio(
        "å›ç­”æ¨¡å¼",
        ["ğŸ§  æ·±åº¦æ€è€ƒ", "âš¡ å¿«é€Ÿç®€æ´"],
        help="æ·±åº¦æ€è€ƒæ›´æ™ºèƒ½ä½†è¾ƒæ…¢ï¼Œå¿«é€Ÿç®€æ´æ›´ç›´æ¥"
    )

    show_debug = st.checkbox("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯", value=False)

    st.divider()

    # çŠ¶æ€
    st.header("ğŸ“Š çŠ¶æ€")
    if st.session_state.pdf_processed:
        st.success("âœ… å°±ç»ª")
        st.caption(f"å¯¹è¯è½®æ•°: {len(st.session_state.chat_history)}")
    else:
        st.warning("â³ ç­‰å¾…ä¸Šä¼ ")

    if st.session_state.chat_history:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
            st.session_state.chat_history = []
            st.session_state.debug_info = {}
            st.rerun()

# ==================== å³ä¾§å¯¹è¯åŒº ====================
with col2:
    st.header("ğŸ’¬ å¯¹è¯")

    # å¯¹è¯å†å²
    for chat in st.session_state.chat_history:
        with st.chat_message("user"):
            st.write(chat["question"])

        with st.chat_message("assistant"):
            st.write(chat["answer"])

            # æ˜¾ç¤ºå‚è€ƒæ¥æº
            with st.expander("ğŸ“– å‚è€ƒæ¥æº"):
                for i, doc in enumerate(chat.get("sources", [])[:3]):
                    st.caption(f"**[{i + 1}]** {doc.page_content[:200]}...")

            # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            if show_debug and "debug" in chat:
                with st.expander("ğŸ”§ è°ƒè¯•ä¿¡æ¯"):
                    debug = chat["debug"]
                    st.write(f"**æ£€ç´¢å…³é”®è¯:** {debug.get('search_queries', [])}")
                    st.write(f"**æ‰¾åˆ°æ–‡æ¡£:** {debug.get('docs_found', 0)} å—")
                    st.write(f"**è´¨é‡æ£€æŸ¥:** {debug.get('quality_check', 'N/A')}")
                    st.write(f"**æ˜¯å¦æ”¹è¿›:** {debug.get('improved', False)}")

    # è¾“å…¥
    if st.session_state.pdf_processed:
        question = st.chat_input("è¾“å…¥é—®é¢˜...")

        if question:
            with st.chat_message("user"):
                st.write(question)

            with st.chat_message("assistant"):
                with st.spinner("æ€è€ƒä¸­..."):
                    try:
                        if "æ·±åº¦" in answer_mode:
                            answer, docs, debug_info = smart_rag_answer(
                                question,
                                st.session_state.vectorstore,
                                st.session_state.chat_history
                            )
                        else:
                            docs, queries = smart_retrieve(
                                question,
                                st.session_state.vectorstore
                            )
                            answer = concise_answer(
                                question,
                                docs,
                                st.session_state.chat_history
                            )
                            debug_info = {"search_queries": queries, "mode": "concise"}

                        st.write(answer)

                        with st.expander("ğŸ“– å‚è€ƒæ¥æº"):
                            for i, doc in enumerate(docs[:3]):
                                st.caption(f"**[{i + 1}]** {doc.page_content[:200]}...")

                        if show_debug:
                            with st.expander("ğŸ”§ è°ƒè¯•ä¿¡æ¯"):
                                st.write(debug_info)

                        # ä¿å­˜
                        st.session_state.chat_history.append({
                            "question": question,
                            "answer": answer,
                            "sources": docs,
                            "debug": debug_info
                        })

                    except Exception as e:
                        st.error(f"å‡ºé”™: {e}")
    else:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼  PDF æ–‡æ¡£")

st.divider()
st.caption("ğŸ”’ æœ¬åœ°å¤„ç† | v3.0 æ™ºèƒ½æ£€ç´¢ + æ€è€ƒå‹å›ç­”")