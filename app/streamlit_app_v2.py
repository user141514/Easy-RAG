# æ–‡ä»¶: app/streamlit_app_v2.py
# RAG çŸ¥è¯†åº“ v2.0 - æ”¯æŒå¯¹è¯è®°å¿† + å…¨æ–‡æ‘˜è¦

import streamlit as st
import tempfile
import os
import shutil
import gc
import time
import uuid

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate

# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="RAG çŸ¥è¯†åº“ v2.0",
    page_icon="ğŸ“š",
    layout="wide"
)

# ==================== å¸¸é‡é…ç½® ====================
VECTORDB_BASE_PATH = r"D:\local-rag-chatbot\data\vectordb"
OLLAMA_BASE_URL = "http://localhost:11434"
LLM_MODEL = "llama3:8b"
EMBEDDING_MODEL = "nomic-embed-text"

# å¯¹è¯è®°å¿†é…ç½®
MAX_HISTORY_TURNS = 5  # ä¿ç•™æœ€è¿‘ 5 è½®å¯¹è¯

# ==================== Session State ====================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False
if "document_chunks" not in st.session_state:
    st.session_state.document_chunks = []
if "document_summary" not in st.session_state:
    st.session_state.document_summary = None


# ==================== æ ¸å¿ƒå‡½æ•° ====================

@st.cache_resource
def get_llm():
    return OllamaLLM(
        model=LLM_MODEL,
        base_url=OLLAMA_BASE_URL
    )


@st.cache_resource
def get_embeddings():
    return OllamaEmbeddings(
        model=EMBEDDING_MODEL,
        base_url=OLLAMA_BASE_URL
    )


def cleanup_old_vectorstore():
    """æ¸…ç†æ—§çš„å‘é‡æ•°æ®åº“è¿æ¥"""
    if st.session_state.vectorstore is not None:
        try:
            st.session_state.vectorstore = None
            gc.collect()
            time.sleep(0.5)
        except:
            pass


def process_pdf(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„ PDF æ–‡ä»¶"""

    cleanup_old_vectorstore()

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    try:
        # åŠ è½½ PDF
        loader = PyPDFLoader(tmp_path)
        pages = loader.load()

        # åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
        )
        chunks = text_splitter.split_documents(pages)

        # åˆ›å»ºå‘é‡æ•°æ®åº“
        db_path = f"{VECTORDB_BASE_PATH}_{uuid.uuid4().hex[:8]}"

        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=get_embeddings(),
            persist_directory=db_path
        )

        return vectorstore, pages, chunks

    finally:
        try:
            os.unlink(tmp_path)
        except:
            pass


def format_chat_history(chat_history, max_turns=MAX_HISTORY_TURNS):
    """æ ¼å¼åŒ–å¯¹è¯å†å²"""
    if not chat_history:
        return "æ— "

    # åªå–æœ€è¿‘ N è½®
    recent_history = chat_history[-max_turns:]

    formatted = []
    for chat in recent_history:
        formatted.append(f"ç”¨æˆ·: {chat['question']}")
        # æˆªæ–­è¿‡é•¿çš„å›ç­”
        answer = chat['answer'][:200] + "..." if len(chat['answer']) > 200 else chat['answer']
        formatted.append(f"åŠ©æ‰‹: {answer}")

    return "\n".join(formatted)


def get_rag_response_with_memory(question: str, vectorstore, chat_history, top_k: int = 3):
    """å¸¦å¯¹è¯è®°å¿†çš„ RAG é—®ç­”"""

    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retrieved_docs = vectorstore.similarity_search(question, k=top_k)
    context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])

    # 2. æ ¼å¼åŒ–å¯¹è¯å†å²
    history_text = format_chat_history(chat_history)

    # 3. æ„å»ºå¸¦è®°å¿†çš„ Prompt
    template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®å‚è€ƒæ–‡æ¡£å’Œå¯¹è¯å†å²æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å‚è€ƒæ–‡æ¡£ã€‘
{context}

ã€å¯¹è¯å†å²ã€‘
{history}

ã€å½“å‰é—®é¢˜ã€‘
{question}

ã€å›ç­”è¦æ±‚ã€‘
1. ç»“åˆå¯¹è¯å†å²ç†è§£ç”¨æˆ·æ„å›¾ï¼ˆå¦‚"ä»–"ã€"å®ƒ"ã€"è¿™ä¸ª"ç­‰ä»£è¯ï¼‰
2. åªæ ¹æ®å‚è€ƒæ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ 
3. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
4. å›ç­”ç®€æ´å‡†ç¡®ï¼Œä½¿ç”¨ä¸­æ–‡

å›ç­”ï¼š"""

    prompt = PromptTemplate(
        input_variables=["context", "history", "question"],
        template=template
    )

    formatted_prompt = prompt.format(
        context=context,
        history=history_text,
        question=question
    )

    response = get_llm().invoke(formatted_prompt)

    return response, retrieved_docs


def summarize_single_chunk(chunk_text: str, chunk_index: int, total_chunks: int):
    """æ‘˜è¦å•ä¸ªæ–‡æ¡£å—"""

    template = """è¯·ç”¨ 2-3 å¥è¯æ¦‚æ‹¬ä»¥ä¸‹æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ï¼š

{text}

æ¦‚æ‹¬ï¼š"""

    prompt = PromptTemplate(
        input_variables=["text"],
        template=template
    )

    formatted_prompt = prompt.format(text=chunk_text)
    summary = get_llm().invoke(formatted_prompt)

    return summary.strip()


def combine_summaries(summaries: list):
    """åˆå¹¶å¤šä¸ªæ‘˜è¦"""

    combined_text = "\n\n".join([f"ç‰‡æ®µ{i + 1}: {s}" for i, s in enumerate(summaries)])

    template = """ä»¥ä¸‹æ˜¯ä¸€ä»½æ–‡æ¡£å„éƒ¨åˆ†çš„æ‘˜è¦ã€‚è¯·å°†å®ƒä»¬æ•´åˆæˆä¸€ä»½å®Œæ•´ã€è¿è´¯çš„æ€»ç»“ï¼ˆ300-500å­—ï¼‰ï¼š

{summaries}

ã€è¦æ±‚ã€‘
1. ä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸»è¦è§‚ç‚¹
2. é€»è¾‘æ¸…æ™°ï¼Œç»“æ„å®Œæ•´
3. ä½¿ç”¨ä¸­æ–‡

ç»¼åˆæ€»ç»“ï¼š"""

    prompt = PromptTemplate(
        input_variables=["summaries"],
        template=template
    )

    formatted_prompt = prompt.format(summaries=combined_text)
    final_summary = get_llm().invoke(formatted_prompt)

    return final_summary.strip()


def generate_document_summary(chunks, progress_callback=None):
    """
    Map-Reduce æ–¹å¼ç”Ÿæˆå…¨æ–‡æ‘˜è¦

    Args:
        chunks: æ–‡æ¡£å—åˆ—è¡¨
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """

    total_chunks = len(chunks)

    # ===== MAP é˜¶æ®µï¼šæ¯ä¸ªå—ç”Ÿæˆæ‘˜è¦ =====
    chunk_summaries = []

    for i, chunk in enumerate(chunks):
        if progress_callback:
            progress_callback(i + 1, total_chunks, "MAP")

        summary = summarize_single_chunk(chunk.page_content, i, total_chunks)
        chunk_summaries.append(summary)

    # ===== REDUCE é˜¶æ®µï¼šåˆå¹¶æ‘˜è¦ =====
    if progress_callback:
        progress_callback(0, 0, "REDUCE")

    # å¦‚æœæ‘˜è¦å¤ªå¤šï¼Œåˆ†æ‰¹åˆå¹¶
    while len(chunk_summaries) > 10:
        batch_size = 5
        new_summaries = []

        for i in range(0, len(chunk_summaries), batch_size):
            batch = chunk_summaries[i:i + batch_size]
            merged = combine_summaries(batch)
            new_summaries.append(merged)

        chunk_summaries = new_summaries

    # æœ€ç»ˆåˆå¹¶
    final_summary = combine_summaries(chunk_summaries)

    return final_summary


# ==================== é¡µé¢å¸ƒå±€ ====================

st.title("ğŸ“š RAG çŸ¥è¯†åº“ v2.0")
st.caption("æ”¯æŒå¯¹è¯è®°å¿† + å…¨æ–‡æ‘˜è¦ | æ•°æ®å®Œå…¨æœ¬åœ°å¤„ç†")

# ä¸‰åˆ—å¸ƒå±€
col1, col2 = st.columns([1, 2])

# ==================== å·¦ä¾§ï¼šæ§åˆ¶é¢æ¿ ====================
with col1:
    st.header("ğŸ“ æ–‡æ¡£ç®¡ç†")

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("é€‰æ‹© PDF", type=["pdf"])

    if uploaded_file:
        st.success(f"å·²é€‰æ‹©: {uploaded_file.name}")

        if st.button("ğŸš€ å¤„ç†æ–‡æ¡£", type="primary", use_container_width=True):
            with st.spinner("å¤„ç†ä¸­..."):
                try:
                    vectorstore, pages, chunks = process_pdf(uploaded_file)
                    st.session_state.vectorstore = vectorstore
                    st.session_state.document_chunks = chunks
                    st.session_state.pdf_processed = True
                    st.session_state.chat_history = []
                    st.session_state.document_summary = None

                    st.success(f"âœ… å®Œæˆï¼{len(pages)} é¡µï¼Œ{len(chunks)} å—")
                except Exception as e:
                    st.error(f"å¤±è´¥: {e}")

    st.divider()

    # ===== å…¨æ–‡æ‘˜è¦åŠŸèƒ½ =====
    st.header("ğŸ“ å…¨æ–‡æ‘˜è¦")

    if st.session_state.pdf_processed:
        if st.session_state.document_summary:
            st.success("âœ… æ‘˜è¦å·²ç”Ÿæˆ")
            with st.expander("æŸ¥çœ‹å…¨æ–‡æ‘˜è¦", expanded=False):
                st.write(st.session_state.document_summary)
        else:
            chunk_count = len(st.session_state.document_chunks)
            st.info(f"æ–‡æ¡£å…± {chunk_count} å—")

            # é¢„ä¼°æ—¶é—´æé†’
            estimated_time = chunk_count * 5  # å‡è®¾æ¯å— 5 ç§’
            st.caption(f"â±ï¸ é¢„è®¡è€—æ—¶: {estimated_time // 60} åˆ† {estimated_time % 60} ç§’")

            if st.button("ğŸ“ ç”Ÿæˆå…¨æ–‡æ‘˜è¦", use_container_width=True):
                progress_bar = st.progress(0)
                status_text = st.empty()


                def update_progress(current, total, phase):
                    if phase == "MAP":
                        progress = current / total * 0.8  # MAP å  80%
                        status_text.text(f"æ­£åœ¨åˆ†æ: {current}/{total} å—")
                    else:
                        progress = 0.8 + 0.2  # REDUCE å  20%
                        status_text.text("æ­£åœ¨æ•´åˆæ‘˜è¦...")
                    progress_bar.progress(progress)


                try:
                    summary = generate_document_summary(
                        st.session_state.document_chunks,
                        progress_callback=update_progress
                    )
                    st.session_state.document_summary = summary
                    progress_bar.progress(1.0)
                    status_text.text("âœ… å®Œæˆï¼")
                    st.rerun()
                except Exception as e:
                    st.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
    else:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£")

    st.divider()

    # ===== çŠ¶æ€æ˜¾ç¤º =====
    st.header("ğŸ“Š çŠ¶æ€")

    if st.session_state.pdf_processed:
        st.success("âœ… æ–‡æ¡£å·²åŠ è½½")
        st.info(f"ğŸ’¬ å¯¹è¯è½®æ•°: {len(st.session_state.chat_history)}")
        st.caption(f"è®°å¿†çª—å£: æœ€è¿‘ {MAX_HISTORY_TURNS} è½®")
    else:
        st.warning("â³ ç­‰å¾…ä¸Šä¼ ")

    # æ¸…ç©ºæŒ‰é’®
    if st.session_state.chat_history:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()

# ==================== å³ä¾§ï¼šå¯¹è¯åŒº ====================
with col2:
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    # æ˜¾ç¤ºå¯¹è¯å†å²
    chat_container = st.container()

    with chat_container:
        if not st.session_state.chat_history:
            if st.session_state.pdf_processed:
                st.info("ğŸ‘‹ æ–‡æ¡£å·²å°±ç»ªï¼Œå¼€å§‹æé—®å§ï¼æ”¯æŒå¤šè½®å¯¹è¯å’Œä»£è¯æŒ‡ä»£ã€‚")

                # ç¤ºä¾‹é—®é¢˜
                st.caption("ğŸ’¡ è¯•è¯•è¿™æ ·é—®ï¼š")
                example_cols = st.columns(2)
                with example_cols[0]:
                    st.caption("â€¢ CEOæ˜¯è°ï¼Ÿ")
                    st.caption("â€¢ ä»–çš„èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆè¿½é—®ï¼‰")
                with example_cols[1]:
                    st.caption("â€¢ å…¬å¸æœ‰ä»€ä¹ˆäº§å“ï¼Ÿ")
                    st.caption("â€¢ ç¬¬ä¸€ä¸ªäº§å“è¯¦ç»†è¯´è¯´ï¼ˆè¿½é—®ï¼‰")
            else:
                st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡æ¡£")
        else:
            for i, chat in enumerate(st.session_state.chat_history):
                with st.chat_message("user"):
                    st.write(chat["question"])

                with st.chat_message("assistant"):
                    st.write(chat["answer"])

                    with st.expander("ğŸ“– å‚è€ƒæ¥æº"):
                        for j, doc in enumerate(chat["sources"]):
                            st.caption(f"**æ¥æº {j + 1}:** {doc.page_content[:150]}...")

    st.divider()

    # è¾“å…¥åŒº
    if st.session_state.pdf_processed:
        question = st.chat_input("è¯·è¾“å…¥é—®é¢˜ï¼ˆæ”¯æŒè¿½é—®ï¼Œå¦‚'ä»–æ˜¯è°'ã€'è¯¦ç»†è¯´è¯´'ï¼‰...")

        if question:
            with st.chat_message("user"):
                st.write(question)

            with st.chat_message("assistant"):
                with st.spinner("æ€è€ƒä¸­..."):
                    try:
                        answer, sources = get_rag_response_with_memory(
                            question,
                            st.session_state.vectorstore,
                            st.session_state.chat_history
                        )
                        st.write(answer)

                        with st.expander("ğŸ“– å‚è€ƒæ¥æº"):
                            for i, doc in enumerate(sources):
                                st.caption(f"**æ¥æº {i + 1}:** {doc.page_content[:150]}...")

                        # ä¿å­˜åˆ°å†å²
                        st.session_state.chat_history.append({
                            "question": question,
                            "answer": answer,
                            "sources": sources
                        })
                    except Exception as e:
                        st.error(f"å‡ºé”™: {e}")
    else:
        st.chat_input("è¯·å…ˆä¸Šä¼ æ–‡æ¡£...", disabled=True)

# ==================== é¡µè„š ====================
st.divider()
st.caption("ğŸ”’ æ‰€æœ‰æ•°æ®æœ¬åœ°å¤„ç† | v2.0 æ”¯æŒå¯¹è¯è®°å¿†å’Œå…¨æ–‡æ‘˜è¦")