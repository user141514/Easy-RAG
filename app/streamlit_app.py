# æ–‡ä»¶: app/streamlit_app.py
# æœ¬åœ° RAG çŸ¥è¯†åº“é—®ç­” - ä¿®å¤æ–‡ä»¶é”å®šé—®é¢˜

import streamlit as st
import tempfile
import os
import shutil
import gc
import time

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate

# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="æœ¬åœ°çŸ¥è¯†åº“é—®ç­”",
    page_icon="ğŸ“š",
    layout="wide"
)

# ==================== å¸¸é‡é…ç½® ====================
VECTORDB_BASE_PATH = r"D:\local-rag-chatbot\data\vectordb"
OLLAMA_BASE_URL = "http://localhost:11434"
LLM_MODEL = "llama3:8b"
EMBEDDING_MODEL = "nomic-embed-text"

# ==================== åˆå§‹åŒ– Session State ====================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False
if "current_db_path" not in st.session_state:
    st.session_state.current_db_path = None


# ==================== æ ¸å¿ƒå‡½æ•° ====================

@st.cache_resource
def get_llm():
    """åˆå§‹åŒ– LLM"""
    return OllamaLLM(
        model=LLM_MODEL,
        base_url=OLLAMA_BASE_URL
    )


@st.cache_resource
def get_embeddings():
    """åˆå§‹åŒ– Embedding æ¨¡å‹"""
    return OllamaEmbeddings(
        model=EMBEDDING_MODEL,
        base_url=OLLAMA_BASE_URL
    )


def cleanup_old_vectorstore():
    """æ¸…ç†æ—§çš„å‘é‡æ•°æ®åº“è¿æ¥"""
    if st.session_state.vectorstore is not None:
        try:
            # æ¸…é™¤å¼•ç”¨
            st.session_state.vectorstore = None
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            # ç­‰å¾…æ–‡ä»¶é‡Šæ”¾
            time.sleep(0.5)
        except Exception as e:
            st.warning(f"æ¸…ç†æ—§æ•°æ®åº“æ—¶å‡ºç°è­¦å‘Š: {e}")


def process_pdf(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„ PDF æ–‡ä»¶"""

    # 1. æ¸…ç†æ—§è¿æ¥
    cleanup_old_vectorstore()

    # 2. ä¿å­˜ä¸Šä¼ æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    try:
        # 3. åŠ è½½ PDF
        loader = PyPDFLoader(tmp_path)
        pages = loader.load()

        # 4. åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
        )
        chunks = text_splitter.split_documents(pages)

        # 5. ç”Ÿæˆå”¯ä¸€çš„æ•°æ®åº“è·¯å¾„ï¼ˆé¿å…é”å®šé—®é¢˜ï¼‰
        import uuid
        db_path = f"{VECTORDB_BASE_PATH}_{uuid.uuid4().hex[:8]}"

        # 6. æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§ç›®å½•
        if os.path.exists(db_path):
            shutil.rmtree(db_path)

        # 7. åˆ›å»ºå‘é‡æ•°æ®åº“
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=get_embeddings(),
            persist_directory=db_path
        )

        # 8. ä¿å­˜å½“å‰æ•°æ®åº“è·¯å¾„
        st.session_state.current_db_path = db_path

        return vectorstore, len(pages), len(chunks)

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.unlink(tmp_path)
        except:
            pass


def get_rag_response(question: str, vectorstore, top_k: int = 3):
    """RAG é—®ç­”"""

    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retrieved_docs = vectorstore.similarity_search(question, k=top_k)

    # æ‹¼æ¥ä¸Šä¸‹æ–‡
    context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])

    # æ„å»º Prompt
    template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å‚è€ƒæ–‡æ¡£ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€å›ç­”è¦æ±‚ã€‘
1. åªæ ¹æ®å‚è€ƒæ–‡æ¡£å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦ç®€æ´ã€å‡†ç¡®
4. ä½¿ç”¨ä¸­æ–‡å›ç­”

å›ç­”ï¼š"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=template
    )

    formatted_prompt = prompt.format(context=context, question=question)
    response = get_llm().invoke(formatted_prompt)

    return response, retrieved_docs


# ==================== é¡µé¢å¸ƒå±€ ====================

st.title("ğŸ“š æœ¬åœ°çŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹")
st.caption("ä¸Šä¼  PDF æ–‡æ¡£ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œæ™ºèƒ½é—®ç­” | æ•°æ®å®Œå…¨æœ¬åœ°å¤„ç†")

col1, col2 = st.columns([1, 2])

# ==================== å·¦ä¾§ï¼šæ–‡ä»¶ä¸Šä¼  ====================
with col1:
    st.header("ğŸ“ æ–‡æ¡£ä¸Šä¼ ")

    uploaded_file = st.file_uploader(
        "é€‰æ‹© PDF æ–‡ä»¶",
        type=["pdf"],
        help="ä¸Šä¼  PDF æ–‡æ¡£"
    )

    if uploaded_file is not None:
        st.success(f"å·²é€‰æ‹©: {uploaded_file.name}")

        if st.button("ğŸš€ å¤„ç†æ–‡æ¡£", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                try:
                    vectorstore, num_pages, num_chunks = process_pdf(uploaded_file)
                    st.session_state.vectorstore = vectorstore
                    st.session_state.pdf_processed = True
                    st.session_state.chat_history = []

                    st.success(f"âœ… å¤„ç†å®Œæˆï¼é¡µæ•°: {num_pages}ï¼Œåˆ†å—: {num_chunks}")
                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {str(e)}")

    st.divider()
    st.header("ğŸ“Š çŠ¶æ€")

    if st.session_state.pdf_processed:
        st.success("âœ… æ–‡æ¡£å·²åŠ è½½")
    else:
        st.warning("â³ ç­‰å¾…ä¸Šä¼ ")

    # æ¸…ç©ºå¯¹è¯æŒ‰é’®
    if st.session_state.chat_history:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()

# ==================== å³ä¾§ï¼šå¯¹è¯åŒº ====================
with col2:
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    chat_container = st.container()

    with chat_container:
        if not st.session_state.chat_history:
            st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡æ¡£")
        else:
            for chat in st.session_state.chat_history:
                with st.chat_message("user"):
                    st.write(chat["question"])

                with st.chat_message("assistant"):
                    st.write(chat["answer"])

                    with st.expander("ğŸ“– å‚è€ƒæ¥æº"):
                        for i, doc in enumerate(chat["sources"]):
                            st.caption(f"**æ¥æº {i + 1}:** {doc.page_content[:150]}...")

    st.divider()

    if st.session_state.pdf_processed:
        question = st.chat_input("è¯·è¾“å…¥é—®é¢˜...")

        if question:
            with st.chat_message("user"):
                st.write(question)

            with st.chat_message("assistant"):
                with st.spinner("æ€è€ƒä¸­..."):
                    try:
                        answer, sources = get_rag_response(
                            question,
                            st.session_state.vectorstore
                        )
                        st.write(answer)

                        with st.expander("ğŸ“– å‚è€ƒæ¥æº"):
                            for i, doc in enumerate(sources):
                                st.caption(f"**æ¥æº {i + 1}:** {doc.page_content[:150]}...")

                        st.session_state.chat_history.append({
                            "question": question,
                            "answer": answer,
                            "sources": sources
                        })
                    except Exception as e:
                        st.error(f"å‡ºé”™: {str(e)}")
    else:
        st.chat_input("è¯·å…ˆä¸Šä¼ æ–‡æ¡£...", disabled=True)

st.divider()
st.caption("ğŸ”’ æ‰€æœ‰æ•°æ®æœ¬åœ°å¤„ç†")